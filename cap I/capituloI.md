Capítulo     1
======
1. Processamento da Linguagem Natural
-------
### 1.1 História

Na Segunda Guerra Mundial, a criação do Colossus pela Grã Betanha para decifrar os códigos de Adolf Hitler, pode ser considerado o primeiro computador moderno a fazer o processamento cifrado de dados. "Estávamos em dúvida sobre o quanto era importante", disse o capitão Jerry Roberts, o único membro sobrevivente da unidade, chamada de Testery, que utilizou Colossus para quebrar códigos. "Nós vimos uma série de mensagens assinadas pelo próprio Adolf Hitler."

Após as duas Guerras Mundiais e com o avanço exponencial da tecnologia durante elas, a Guerra Fria veio desenvolver ainda mais a análise da linguagem. O medo da guerra nuclear e de espiões soviéticos roubando informações provocou o desenvolvimento do Processamento de Linguagem Natural a partir da tradução do idioma russo, falado e escrito, para o inglês. Mais na frente sendo base para aplicações, como IBM Watson e o moderno aplicativo Apple Siri.

Foi no âmbito caótico e perturbador de uma guerra que o processamento da linguagem natural foi inventado. Essa ferramenta foi considerada de extrema importância para o governo dos Estados Unidos porque traduzia com quase cem por cento de eficiência um texto escrito em russo para o inglês em velocidade muito superior à qualquer encarregado da tradução. Em demonstração do IBM Georgetown o equipamento “converteu mais de 60 frases de russo para o inglês usando o computador mainframe IBM-701. Seu método de tradução automática foi usar linguística computacional, uma combinação de estatísticas e regras da linguagem.”

“Até o final dos anos 1950 e início dos anos 1960, o processamento da fala e da linguagem se separaram em dois paradigmas: simbólico e estocástico. O paradigma simbólico decolou de duas linhas de pesquisa. O primeiro foi a obra de Chomsky e outras sobre teoria da linguagem formal e sintaxe gerativa em todo o final dos anos 1950 e início e meio dos anos 1960, e o trabalho de lingüística e muitos computadores cientistas sobre algoritmos de análise, inicialmente top-down e bottom-up e depois com dinâmica programação.”

No período de 1954 a 1966 o campo da inteligencia artificial sofreu uma enorme contribuição de pesquisadores e estudantes. Em 1957, “Noam Chomsky publicou as Estruturas Sintáticas, livro que ele revolucionou os conceitos anteriores da linguística e concluiu que, para que as máquinas entendessem linguagem humana a estrutura da frase tinha que mudar. Chomsky, então, inventou um tipo de gramática intitulado Gramática Fase Estruturada que, metodicamente, convertia frases em linguagem natural para uma forma utilizável por computadores.” 

Com as previsões positivas para um crescimento de larga escala versus a falta de recursos técnicos computacionais, deu-se um paradoxo no progresso, a tradução humana passou a ser mais viável do que a tradução automatizda. Os financiamentos já não cobriam os gastos e passou-se a precisar de mais dinheiro para pesquisas. Por isso que em 1966, alguns estudiosos dizem que foi uma época obscura para ciência sobre o PLN, pois os financiamentos pararam e a esperança nas máquinas de tradução foi diminuindo. Demorou pouco mais de 14 anos para recuperação da confiança. Com computadores mais modernos, o processamento de dados se viu em outro patamar, os chatterbots. Criado em 1982, o Jabberwacky foi um programa de inteligência artificial que simulava uma conversa natural de forma interessante, divertida e bem-humorada. Este projeto reforçou ainda mais as pesquisas e a confiança no processamento da linguagem natural.

A partir de 90, o crescimento do PLN junto ao desenvolvimento da World Wide Web tornou-se rápido e de extrema importância. Com a abundância e o surgimento de textos, artigos, revisões e todas as formas de opiniões escritas a todo instante, a ciência do processamento da linguagem se viu no papel de classificar boa parte das informações contidas na web e guardar num imenso banco de dados. Já no século 21 valendo-se dessa ideia, a companhia International Business Machines, IBM, em 2011 construiu o supercomputador Watson que processava a linguagem humana e interpretava de maneira sensata. Foi colocado em prática no programa de perguntas e respostas Jeopardy!, e derrotou os campeões, Brad Rutter e Ken Jennings.

Atualmente o Processamento de Linguagem Natural moderno consiste no estudo do reconhecimento de voz, aprendizagem de máquina (machine learning), máquina de leitura de texto e ferramentas mais avançadas para traduções automáticas. Estes, quando associados permitem à inteligência artificial adquirir conhecimento real e verdadeiro do mundo, e não apenas situações de predições de jogadas como num jogo de xadrez. “Nos computadores do futuro próximo será capaz de ler todas as informações on-line e aprender com ele e resolver problemas e, possivelmente, curar doenças”.

### 1.2 Python e Natural Language Toolkit 
Todos os exemplos dessa pesquisa são feitos em Python em conjunto com a biblioteca de processamento de linguagem natural NLTK. Nas seções abaixo deste capítulo demonstraremos porquê utilizaremos essas tecnologias.

### 1.2.1 Por que Python?
Python é uma linguagem de programação denominada altíssimo nível criada por Guido Van Rossum. Adquiriu este patamar por sua sintaxe ser bastante parecida com a fala humana. Seus códigos são bem semelhantes como uma redação em inglês. (LABAKI, 2003) destaca que este ideal fez com que o desenvolvimento de Python tivesse sempre em mente a liberdade (gratuita, código aberto), disponibilidade (Python roda em Windows, Linux, Mac, Palm, em celulares e outras infinidades de sistemas) e principalmente a clareza de sintaxe, que hoje é responsável pela alta produtividade só conseguida com Python.

Segundo (BORGES, 2010) outro destaque é que a linguagem inclui diversas estruturas de alto nível (listas, dicionários, data/hora e outras) e uma vasta coleção de módulos prontos para uso, além de frameworks de terceiros que podem ser adicionados. Também possui recursos encontrados em outras linguagens modernas, tais como: geradores, introspecção, persistência, metaclasses e unidades de teste. Multiparadigma, a linguagem suporta programação modular e funcional, além da orientação a objetos. Mesmo os tipos básicos no Python são objetos. A linguagem é interpretada através de bytecode pela máquina virtual Python, tornando o código portável.
    
### 1.2.2 E por que NLTK?

O NLTK, Natural Language Toolkit, é um conjunto de módulos Python que fornece muitos tipos de dados do Processamento da Linguagem Natural como, amostra de corpus e leitores de processamento em conjunto com algoritmos de animação, plotagem de gráficos, tutoriais e conjuntos de problemas. (LOPER e BIRD, 2002).

Criado em 2001 como parte de um curso de linguística computacional de uma universidade na Pensilvânia, o NLTK, tem sido desenvolvido e expandido com a ajuda de dezenas de colaboradores e estudantes. Essa ferramenta está sendo adotada em muitas faculdades como base para a matéria de linguística e como módulo principal para projetos na área da Linguagem Natural.

O NLTK foi designado com quatro objetivos primários. Simplicidade: provendo um framework intuitivo e dando um conhecimento prático do Processamento de Linguagem Natural. Consistência: para fornecer um quadro uniforme com interfaces consistentes, estruturas de dados e nomes de métodos de fáceis. Extensibilidade: promovendo uma estrutura na qual novos módulos de softwares podem ser facilmente acomodados. Modularidade: para fornecer componentes que podem ser usados independentemente sem a necessidade de compreender toda a ferramenta. (Natural Language Processing with Python, 2009, p. XV)